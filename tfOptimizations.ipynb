{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfOptimizations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenzwj/Kaggle-Competition-Favorita/blob/master/tfOptimizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsYrLKRGOebx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "394665b0-5047-4a57-8355-454e79885cc7"
      },
      "source": [
        "import contextlib\n",
        "import functools\n",
        "import gc\n",
        "import multiprocessing\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import time\n",
        "import timeit\n",
        "import urllib.request\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "with tf.device(\"GPU:0\"):\n",
        "  tf.ones(())  # Make sure we can run on GPU\n",
        "\n",
        "data_root = \"/tmp/demo_images\"\n",
        "profile_dir = os.path.join(data_root, \"profiles\")\n",
        "os.makedirs(profile_dir, exist_ok=True)\n",
        "\n",
        "# This ensures that XLA and ptxas work well together, and helps with scaling.\n",
        "print(\"XLA_FLAGS='{}'\".format(os.getenv(\"XLA_FLAGS\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-dev20191028\n",
            "Num GPUs Available:  8\n",
            "XLA_FLAGS='--xla_gpu_cuda_data_dir=/usr/local/cuda'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlbpK_YudZBO"
      },
      "source": [
        "## Configure task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV3APg_xdWdm"
      },
      "source": [
        "RESOLUTION = (224, 224)\n",
        "NUM_CHANNELS = 3\n",
        "\n",
        "num_images_per_label = 50000\n",
        "NUM_TOTAL_IMAGES = num_images_per_label * 2\n",
        "\n",
        "pos_dir = os.path.join(data_root, \"positives\")\n",
        "neg_dir = os.path.join(data_root, \"negatives\")\n",
        "download_failures = os.path.join(data_root, \"failed_to_download\")\n",
        "tf_record_dir = os.path.join(data_root, \"tfrecord_data\")\n",
        "tf_record_array_dir = os.path.join(data_root, \"tfrecord_array_data\")\n",
        "\n",
        "os.makedirs(pos_dir, exist_ok=True)\n",
        "os.makedirs(neg_dir, exist_ok=True)\n",
        "os.makedirs(download_failures, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0CdHWMzZZ"
      },
      "source": [
        "## Download the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcTLdyOSA8vg"
      },
      "source": [
        "### Grab OpenImages metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ya55DhVdP3q"
      },
      "source": [
        "def prepackaged_data():\n",
        "  \"\"\"Grab 100,000 prepackaged images.\n",
        "\n",
        "  For convenience, a set of 50,000 positives and 50,000 negatives has been \n",
        "  prepackaged to reduce the time to get up and running.\n",
        "  \"\"\"\n",
        "  if (len(os.listdir(pos_dir)) >= 50000 and\n",
        "      len(os.listdir(neg_dir)) >= 50000):\n",
        "    return  # Already done.\n",
        "\n",
        "  PREPACKAGED_DATA_TAR = os.path.join(data_root, \"tf_world_classifier_data.tar.gz\")\n",
        "  if not os.path.exists(PREPACKAGED_DATA_TAR):\n",
        "    print(\"Downloading tarball...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://download.tensorflow.org/models/official/tf_world_classifier_data.tar.gz\",\n",
        "        PREPACKAGED_DATA_TAR)\n",
        "\n",
        "    try:\n",
        "      cwd = os.getcwd()\n",
        "      os.chdir(data_root)\n",
        "      print(\"Extracting files...\")\n",
        "      with tarfile.open(PREPACKAGED_DATA_TAR) as f:\n",
        "        f.extractall()\n",
        "    finally:\n",
        "      os.chdir(cwd)\n",
        "\n",
        "  for label in [\"positives\", \"negatives\"]:\n",
        "    for i in os.listdir(os.path.join(data_root, \"tf_world_classifier_data\", label)):\n",
        "      destination = os.path.join(data_root, label, i)\n",
        "      if not os.path.exists(destination):\n",
        "        shutil.copy(os.path.join(data_root, \"tf_world_classifier_data\", label, i),\n",
        "                    destination)\n",
        "\n",
        "prepackaged_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_3wHDXBIob"
      },
      "source": [
        "### Register positive labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N21SKrVbXt1T"
      },
      "source": [
        "cat_types = \"\"\"/m/01yrx,Cat\n",
        "/m/012c9l,Domestic short-haired cat\n",
        "/m/012k6q,Ocicat\n",
        "/m/0172jz,Bobcat\n",
        "/m/017fsk,Norwegian forest cat\n",
        "/m/01nq_x,Wild cat\n",
        "/m/01pdqb,Rusty-spotted cat\n",
        "/m/01qpsq,Polecat\n",
        "/m/02j4fs,Madagascar cat\n",
        "/m/02rcwpb,Malayan cat\n",
        "/m/03bw_6d,Tom cat\n",
        "/m/03c_kl,Snowshoe cat\n",
        "/m/03c_ndy,Aegean cat\n",
        "/m/03dj64,Black cat\n",
        "/m/0409r1,Saber-toothed cat\n",
        "/m/06jdsz,Rex cat\n",
        "/m/07k6w8,Small to medium-sized cats\n",
        "/m/08x9c0,Polydactyl cat\n",
        "/m/0cdnk,Big cats\n",
        "/m/0g4cd0,Tabby cat\n",
        "/m/0gvvmf6,Napoleon cat\n",
        "/m/0k8hs,Domestic long-haired cat\"\"\"\n",
        "cat_types_set = {i.split(\",\")[0] for i in cat_types.splitlines()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoLGSu_WdzQZ"
      },
      "source": [
        "### Download raw image files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cYOj8Khcp7G"
      },
      "source": [
        "def maybe_log(i, cadence, max_len):\n",
        "  i += 1\n",
        "  if i % cadence and i != max_len:\n",
        "    return\n",
        "  print(\"{:>8} / {}, ({:.1f}%)\".format(i, max_len, i / max_len * 100))\n",
        "\n",
        "def download_data():\n",
        "  if (len(os.listdir(pos_dir)) >= num_images_per_label and\n",
        "      len(os.listdir(neg_dir)) >= num_images_per_label):\n",
        "    return  # Already done.\n",
        "\n",
        "  if os.path.exists(tf_record_dir):\n",
        "    # TFRecords will have to be regenerated.\n",
        "    shutil.rmtree(tf_record_dir)\n",
        "\n",
        "  TRAIN_IMAGES_METADATA_PATH = tf.keras.utils.get_file(\n",
        "    os.path.join(data_root, \"train-images-with-labels-with-rotation.csv\"),\n",
        "    \"https://storage.googleapis.com/openimages/v5/train-images-with-labels-with-rotation.csv\")\n",
        "  TRAIN_ANNOTATIONS_PATH = tf.keras.utils.get_file(\n",
        "      os.path.join(data_root, \"train-annotations-human-imagelabels.csv\"),\n",
        "      \"https://storage.googleapis.com/openimages/v5/train-annotations-human-imagelabels.csv\")\n",
        "\n",
        "  print(\"Loading metadata CSVs...\")\n",
        "  train_images = pd.read_csv(TRAIN_IMAGES_METADATA_PATH)\n",
        "  train_annotations = pd.read_csv(TRAIN_ANNOTATIONS_PATH)\n",
        "\n",
        "  print(\"Parsing labels...\")\n",
        "  id_is_cat = set()\n",
        "  for i, row in enumerate(train_annotations.itertuples()):\n",
        "    maybe_log(i, int(5e6), len(train_annotations))\n",
        "    if row.Confidence and row.LabelName in cat_types_set:\n",
        "      id_is_cat.add(row.ImageID)\n",
        "\n",
        "  print(\"{} images with cats detected.\".format(len(id_is_cat)))\n",
        "\n",
        "  print(\"Extracting URLs...\")\n",
        "  thumbnail_urls = {True: [], False: []}\n",
        "  for i, row in enumerate(train_images.itertuples()):\n",
        "    maybe_log(i, int(1e6), len(train_images))\n",
        "    if not isinstance(row.Thumbnail300KURL, str):\n",
        "      continue\n",
        "  \n",
        "    thumbnail_urls[row.ImageID in id_is_cat].append(row.Thumbnail300KURL)\n",
        "    \n",
        "  del train_images\n",
        "  del train_annotations\n",
        "\n",
        "  print(\"Selecting data...\")\n",
        "  np.random.seed(0)\n",
        "  shuffled_positive_urls = np.random.permutation(thumbnail_urls[True])\n",
        "  shuffled_negative_urls = np.random.permutation(thumbnail_urls[False])\n",
        "  for urls, dest in [(shuffled_positive_urls, pos_dir), (shuffled_negative_urls, neg_dir)]:\n",
        "    failures = 0\n",
        "    successes = 0\n",
        "    for i, url in enumerate(urls):\n",
        "      if successes >= num_images_per_label:\n",
        "        print(\"Successfully downloaded {} examples\".format(num_images_per_label))\n",
        "        break\n",
        "\n",
        "      filename = os.path.split(url)[1]\n",
        "      if filename.endswith(\"?zz=1\"):\n",
        "        filename = filename[:-5]\n",
        "\n",
        "      if os.path.exists(os.path.join(download_failures, filename)):\n",
        "        continue  # An earlier run failed to download the file.\n",
        "\n",
        "      fpath = os.path.join(dest, filename)\n",
        "      if os.path.exists(fpath):\n",
        "        successes += 1\n",
        "        continue\n",
        "      \n",
        "      try:\n",
        "        urllib.request.urlretrieve(url, fpath)\n",
        "        successes += 1\n",
        "      except urllib.request.HTTPError:\n",
        "        open(os.path.join(download_failures, filename), \"w\").close()\n",
        "        failures += 1\n",
        "        if failures > 10 and failures / (i + 1) > 0.1:\n",
        "          raise # Too many failures.\n",
        "        continue  # Thumbnails are not guaranteed to exist.\n",
        "\n",
        "      if not (i + 1) % 100:\n",
        "        print(\"\\r{}\".format(i + 1), end=\"\")\n",
        "    print()\n",
        "\n",
        "download_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tBbU-ZL9v4r"
      },
      "source": [
        "### Convert to TFRecords (Optional. JPEGs can be used directly.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHmNLgMj9vL5"
      },
      "source": [
        "def get_paths_and_labels():\n",
        "  return (\n",
        "      [(os.path.join(pos_dir, i), 1) for i in os.listdir(pos_dir)] + \n",
        "      [(os.path.join(neg_dir, i), 0) for i in os.listdir(neg_dir)])\n",
        "\n",
        "def write_to_tfrecords(decoded_resolution=None):\n",
        "  if decoded_resolution:\n",
        "    record_dir = os.path.join(tf_record_array_dir, str(decoded_resolution))\n",
        "  else:\n",
        "    record_dir = tf_record_dir\n",
        "\n",
        "  if os.path.exists(record_dir):\n",
        "    return\n",
        "  os.makedirs(record_dir, exist_ok=True)\n",
        "\n",
        "  print(\"Converting images to TFRecords...\")\n",
        "  records_per_shard = 50\n",
        "\n",
        "  shard_number = 0\n",
        "  path_template = os.path.join(record_dir, \"shard_{0:04d}.tfrecords\")\n",
        "  writer = tf.io.TFRecordWriter(path_template.format(shard_number))\n",
        "  for i, (image_path, label) in enumerate(get_paths_and_labels()):\n",
        "    if i and not (i % records_per_shard):\n",
        "      shard_number += 1\n",
        "      writer.close()\n",
        "      writer = tf.io.TFRecordWriter(path_template.format(shard_number))\n",
        "\n",
        "    with open(image_path, \"rb\") as f:\n",
        "      image_bytes = f.read()\n",
        "\n",
        "    if decoded_resolution:\n",
        "      # TODO(robieta): make this faster with imap\n",
        "      image = tf.io.decode_jpeg(image_bytes)\n",
        "      image = tf.cast(image, tf.float32)\n",
        "      image = tf.image.resize(image, (decoded_resolution,) * 2)\n",
        "      if image.shape[2] == 1:\n",
        "        image = tf.tile(image, (1, 1, 3))\n",
        "      image_bytes = tf.io.encode_jpeg(\n",
        "        tf.cast(image, tf.uint8)\n",
        "      ).numpy()\n",
        "\n",
        "\n",
        "    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "        \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes])),\n",
        "        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
        "    })).SerializeToString()\n",
        "\n",
        "    writer.write(record_bytes)\n",
        "\n",
        "  writer.close()\n",
        "  print(\"TFRecord conversion complete.\")\n",
        "\n",
        "\n",
        "RECORD_PATTERN = os.path.join(tf_record_dir, \"*.tfrecords\")\n",
        "RESIZED_RECORD_PATTERN = os.path.join(tf_record_array_dir, \"{}\", \"*.tfrecords\")\n",
        "RECORD_SCHEMA = {\n",
        "    \"image\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
        "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "write_to_tfrecords()\n",
        "\n",
        "assert RESOLUTION[0] == RESOLUTION[1], \"Resize is hard coded to square images.\"\n",
        "write_to_tfrecords(RESOLUTION[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81WaEDOSPSfC"
      },
      "source": [
        "## Model and data setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugyuOzS4O_Tn"
      },
      "source": [
        "def get_input_shape():\n",
        "  if tf.keras.backend.image_data_format() == \"channels_last\":\n",
        "    return RESOLUTION + (NUM_CHANNELS,)\n",
        "  elif tf.keras.backend.image_data_format() == \"channels_first\":\n",
        "    return (NUM_CHANNELS,) + RESOLUTION\n",
        "  raise ValueError(\"Unknown format.\")\n",
        "\n",
        "\n",
        "# Native jpg layout.\n",
        "NHWC_INPUT_SHAPE = RESOLUTION + (NUM_CHANNELS,)\n",
        "\n",
        "\n",
        "def transform_image(image):\n",
        "  image = image / 255.0  # Scale occurs in random transformation\n",
        "  \n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  image += tf.random.normal(tf.shape(image), mean=-0.5, stddev=0.1, dtype=image.dtype)\n",
        "  return image\n",
        "\n",
        "\n",
        "def make_model(input_dtype=tf.float32, transform_on_device=False):\n",
        "  input_shape = get_input_shape()\n",
        "  input_layer = tf.keras.layers.Input(shape=input_shape, dtype=input_dtype)\n",
        "  backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
        "      include_top=False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape=input_shape,\n",
        "      pooling=\"avg\",\n",
        "  )\n",
        "  layer = input_layer\n",
        "  if transform_on_device:\n",
        "    layer = tf.keras.layers.Lambda(transform_image)(layer)\n",
        "\n",
        "  layer = backbone(layer)\n",
        "  final = tf.keras.layers.Dense(1, activation=None)(layer)\n",
        "\n",
        "  return tf.keras.models.Model(input_layer, final, name=\"cat_classifier\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlB2GqBKvWJH"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "But wait, this is much more complicated than the slides...\n",
        "\n",
        "Indeed. This is because it runs all of the options discussed, and tries to clean up after itsef (Hence the context managers), profiles, and provide accurate steady state measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIQenvZd-q93"
      },
      "source": [
        "### Toggles for various optimizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUacwlUg-1pe"
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def stop_profiler():\n",
        "  \"\"\"Used to guarantee that the TensorFlow profiler does not remain on.\n",
        "  \n",
        "  We don't want to mix traces from different runs as it would make them hard\n",
        "  to interpret, so this ensures that the profiler is disabled even if our\n",
        "  training loop crashes.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    yield\n",
        "  finally:\n",
        "    tf.summary.trace_off()\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def use_mixed_precision(loss_scale):\n",
        "  \"\"\"Enable mixed precision, and reset the policy after training.\"\"\"\n",
        "  old_policy = tf.keras.mixed_precision.experimental.global_policy()\n",
        "\n",
        "  try:\n",
        "    policy = tf.compat.v2.keras.mixed_precision.experimental.Policy(\n",
        "        \"mixed_float16\", loss_scale=loss_scale)\n",
        "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    yield\n",
        "  finally:\n",
        "    tf.keras.mixed_precision.experimental.set_policy(old_policy)\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def enable_xla():\n",
        "  \"\"\"Enable XLA, and disable it after training is complete.\"\"\"\n",
        "  try:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    yield\n",
        "  finally:\n",
        "    tf.config.optimizer.set_jit(False)\n",
        "\n",
        "\n",
        "_THREADS_PER_GPU = 2\n",
        "@contextlib.contextmanager\n",
        "def tuning_context():\n",
        "  \"\"\"Hand tuned model configurations.\n",
        "  \n",
        "  Historically these knobs have improved performance, but as of 10/28/2019 they\n",
        "  actually hurt performance. However they are provided simply for completeness\n",
        "  to show some of the lower level knobs.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    os.environ['TF_GPU_THREAD_MODE'] = \"gpu_private\"\n",
        "    os.environ['TF_GPU_THREAD_COUNT'] = str(_THREADS_PER_GPU)\n",
        "    tf.keras.backend.set_image_data_format(\"channels_first\")\n",
        "    yield\n",
        "  finally:\n",
        "    os.environ.pop('TF_GPU_THREAD_MODE', None)\n",
        "    os.environ.pop('TF_GPU_THREAD_COUNT', None)\n",
        "    tf.keras.backend.set_image_data_format(\"channels_last\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq14j9KkqG8z"
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def null_context():\n",
        "  \"\"\"Implementation detail. Used if a given toggle is disabled.\"\"\"\n",
        "  yield\n",
        "\n",
        "def null_decorator(f):\n",
        "  \"\"\"Implementation detail. Used if tf.function is disabled.\"\"\"\n",
        "  return f\n",
        "\n",
        "def train_model(data_fn, global_batch_size, use_tf_function=False, \n",
        "                strategy=None, xla=False, mixed_precision=False, \n",
        "                loss_scale=\"dynamic\", collect_profile=False, tuned=False, \n",
        "                transform_on_device=False):\n",
        "\n",
        "  # Ensure runs are independent.\n",
        "  tf.keras.backend.clear_session()\n",
        "  gc.collect()\n",
        "  time.sleep(3)\n",
        "\n",
        "  if tuned:\n",
        "    assert strategy, \"Tuned version assumes a distribuation strategy is present.\"\n",
        "\n",
        "  dtype = tf.float16 if mixed_precision else tf.float32\n",
        "  step_decorator = tf.function if use_tf_function and not strategy else null_decorator\n",
        "  strategy_scope = strategy.scope() if strategy else null_context()\n",
        "  xla_scope = enable_xla() if xla else null_context()\n",
        "  precision_scope = use_mixed_precision(loss_scale) if mixed_precision else null_context()\n",
        "  tuning_scope = tuning_context() if tuned else null_context()\n",
        "\n",
        "  with strategy_scope, xla_scope, precision_scope, stop_profiler(), tuning_scope:\n",
        "    model = make_model(input_dtype=dtype, transform_on_device=transform_on_device)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "    if mixed_precision:\n",
        "      optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale)\n",
        "\n",
        "    @step_decorator\n",
        "    def replica_step(features, labels):\n",
        "      with tf.GradientTape() as tape:\n",
        "        logits = model(features, training=True)\n",
        "        replica_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)\n",
        "        loss = tf.nn.compute_average_loss(replica_loss, global_batch_size=global_batch_size)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      return loss\n",
        "\n",
        "    step_fn = replica_step\n",
        "    if strategy and use_tf_function:\n",
        "      @tf.function\n",
        "      def replicated_step(features, labels):\n",
        "        loss = strategy.experimental_run_v2(replica_step, (features, labels))\n",
        "        return loss\n",
        "\n",
        "      step_fn = replicated_step\n",
        "\n",
        "    if strategy:\n",
        "      dataset = data_fn(batch_size=global_batch_size, dtype=dtype, \n",
        "                        transform_on_device=transform_on_device)\n",
        "      if tuned:\n",
        "        options = tf.data.Options()\n",
        "        private_threads = (multiprocessing.cpu_count() - \n",
        "                           strategy.num_replicas_in_sync * (_THREADS_PER_GPU + 1))\n",
        "        options.experimental_threading.private_threadpool_size = private_threads\n",
        "        dataset = dataset.with_options(options)\n",
        "      data = strategy.experimental_distribute_dataset(dataset)\n",
        "    else:\n",
        "      assert not transform_on_device\n",
        "      data = data_fn(batch_size=global_batch_size, dtype=dtype)\n",
        "\n",
        "    schedule = [\n",
        "        5,                             # Burn in\n",
        "        5 if collect_profile else 0,   # Profiling\n",
        "        30,                            # Steady state throughput\n",
        "    ]\n",
        "    times = []\n",
        "    \n",
        "    for step_number, inputs in enumerate(data):\n",
        "      loss = step_fn(*inputs)\n",
        "\n",
        "      # Burn in\n",
        "      if schedule[0]:\n",
        "        schedule[0] -= 1\n",
        "        if not schedule[0]:\n",
        "          print(\"Burn in complete.\")\n",
        "          if schedule[1]:\n",
        "            time.sleep(2)  # Let running ops finish to start from a clean trace.\n",
        "            tf.summary.trace_on(profiler=True)\n",
        "          else:\n",
        "            # Skip straight to steady state throughput\n",
        "            start_time = timeit.default_timer()\n",
        "            iter_count = 0\n",
        "        continue\n",
        "\n",
        "      # Op profiler\n",
        "      if schedule[1]:\n",
        "        schedule[1] -= 1\n",
        "        if not schedule[1]:\n",
        "          tf.summary.trace_export(name=\"my_trace\", profiler_outdir=profile_dir)\n",
        "          start_time = timeit.default_timer()\n",
        "          iter_count = 0\n",
        "        continue\n",
        "\n",
        "      # Profile steady state execution\n",
        "      schedule[2] -= 1\n",
        "      iter_count += 1\n",
        "      times.append(timeit.default_timer())\n",
        "      if not schedule[2]:\n",
        "        break\n",
        "\n",
        "    run_time = timeit.default_timer() - start_time\n",
        "    step_time = run_time / iter_count\n",
        "    # print(np.array(times[1:]) - np.array(times[:-1]))\n",
        "    print(\"{} steps\".format(iter_count))\n",
        "    print(\"Mean step time: {:>6.2f} sec\".format(step_time))\n",
        "    print(\"Images / sec:   {:>6d}\".format(int(global_batch_size / step_time)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAYjN5yoNF9h"
      },
      "source": [
        "## First pass at a training function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzeWWSDYg084"
      },
      "source": [
        "### Define a generator based data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eClYVDBxPOR8"
      },
      "source": [
        "def random_flip(image):\n",
        "  hflip = np.random.random() > 0.5\n",
        "  vflip = np.random.random() > 0.5\n",
        "  if hflip and vflip:\n",
        "    image = cv2.flip(image, -1)\n",
        "  elif hflip:\n",
        "    image = cv2.flip(image, -1)\n",
        "  elif vflip:\n",
        "    image = cv2.flip(image, 1)\n",
        "  return image\n",
        "\n",
        "def normalize_and_add_noise(image):\n",
        "  image = image.astype(np.float32) / 255 - 0.5\n",
        "  image += np.random.normal(loc=0, scale=0.1, size=image.shape)\n",
        "  return image\n",
        "\n",
        "def make_batch(features, labels):\n",
        "  x = tf.convert_to_tensor(np.stack(features, axis=0))\n",
        "  y = tf.convert_to_tensor(np.array(labels, dtype=np.float32)[:, np.newaxis])\n",
        "  features.clear()\n",
        "  labels.clear()\n",
        "  return x, y\n",
        "\n",
        "def data_generator(batch_size, **kwargs):\n",
        "  epoch_order = np.random.permutation(get_paths_and_labels())\n",
        "  features, labels = [], []\n",
        "  for image_path, label in epoch_order:\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Resize to training resolution\n",
        "    image = cv2.resize(image, RESOLUTION)\n",
        "\n",
        "    # Randomly horizontal and vertical flip\n",
        "    image = random_flip(image)\n",
        "\n",
        "    # Normalize, center, and add Gaussian noise\n",
        "    image = normalize_and_add_noise(image)\n",
        "    \n",
        "    features.append(image)\n",
        "    labels.append(label)\n",
        "    if len(features) == batch_size:\n",
        "      yield make_batch(features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnEMGFP1egej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "a651a152-1d3b-41ad-f47d-fa58740cc988"
      },
      "source": [
        "for batch_size in [32, 64, 128]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(data_generator, batch_size)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 32\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.48 sec\n",
            "Images / sec:       66\n",
            "\n",
            "Batch size: 64\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.85 sec\n",
            "Images / sec:       75\n",
            "\n",
            "Batch size: 128\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   1.60 sec\n",
            "Images / sec:       80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNqkLxasBDY0"
      },
      "source": [
        "## Add tf.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3pznH5zSHxq"
      },
      "source": [
        "### Use the images directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvrj6QNYSOaU"
      },
      "source": [
        "def make_jpg_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
        "                     already_resized=False):\n",
        "  if already_resized:\n",
        "    raise NotImplementedError(\n",
        "        \"`already_resized` is only implemented for the TFRecords path.\")\n",
        "\n",
        "  def get_bytes_and_label(filepath):\n",
        "    image_bytes = tf.io.read_file(filepath)\n",
        "    label = tf.strings.regex_full_match(filepath, pos_dir + \".+\")\n",
        "    return image_bytes, tf.expand_dims(label, 0)\n",
        "\n",
        "  def process_image(image_bytes, label):\n",
        "    image = tf.io.decode_jpeg(image_bytes)\n",
        "    image = tf.cast(image, dtype)\n",
        "    image = tf.image.resize(image, RESOLUTION)\n",
        "\n",
        "    if tf.shape(image)[2] == 1:\n",
        "      # Some images are greyscale.\n",
        "      image = tf.tile(image, (1, 1, 3))\n",
        "\n",
        "    image.set_shape(NHWC_INPUT_SHAPE)\n",
        "\n",
        "    if not transform_on_device:\n",
        "      image = transform_image(image)\n",
        "\n",
        "    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
        "      image = tf.transpose(image, (2, 0, 1))\n",
        "    \n",
        "    return image, tf.cast(label, dtype)\n",
        "\n",
        "  dataset = tf.data.Dataset.list_files([pos_dir + \"/*\", neg_dir + \"/*\"])\n",
        "  dataset = dataset.shuffle(NUM_TOTAL_IMAGES)\n",
        "  dataset = dataset.map(get_bytes_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Frb0qbSLrG"
      },
      "source": [
        "### Use TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZQvIamLw87p"
      },
      "source": [
        "def make_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
        "                 already_resized=False):\n",
        "  def parse_fn(record):\n",
        "    record = tf.io.parse_single_example(record, RECORD_SCHEMA)\n",
        "    image = tf.io.decode_jpeg(record[\"image\"])\n",
        "    image = tf.cast(image, dtype)\n",
        "    if not already_resized:\n",
        "      image = tf.image.resize(image, RESOLUTION)\n",
        "\n",
        "    if tf.shape(image)[2] == 1:\n",
        "      # Some images are greyscale.\n",
        "      image = tf.tile(image, (1, 1, 3))\n",
        "\n",
        "    image.set_shape(NHWC_INPUT_SHAPE)\n",
        "\n",
        "    if not transform_on_device:\n",
        "      image = transform_image(image)\n",
        "\n",
        "    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
        "      image = tf.transpose(image, (2, 0, 1))\n",
        "    \n",
        "    return image, tf.cast(record[\"label\"], dtype)\n",
        "\n",
        "  pattern = (\n",
        "      RESIZED_RECORD_PATTERN.format(RESOLUTION[0]) if already_resized\n",
        "      else RECORD_PATTERN)\n",
        "\n",
        "  dataset = tf.data.Dataset.list_files(pattern)\n",
        "  dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(4 * batch_size)\n",
        "  dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttFWvxziZap4"
      },
      "source": [
        "### *already_resized* and *transform_on_device*\n",
        "\n",
        "Even with maximum parallelization, the CPU can only produce a bit over 3000 examples per second. This is fine for 1 GPU training since the GPU maxes out in the low 2000's, but would prevent reasonable scaling to more GPUs. This is due to two principle bottlenecks:\n",
        "\n",
        "#### Native image size\n",
        "\n",
        "The downloaded thumbnails tend to be around 400x600 resolution, whereas we're training at 224x224. This means that we have to move approximately 6x as many bytes into memory, spend a correspondingly long time decoding the jpg's, and incur an extra memcpy for the resize. It turns out to be quite important to resize the images to 224x224 and use those resized images in the input pipeline.\n",
        "\n",
        "#### Random augmentation\n",
        "\n",
        "The CPU simply cannot add noise to the images quickly enough to keep up with the GPU, so to maintain performance for the multi-GPU case we have to move those transformations from the input pipeline to the start of the model. Even though that puts them on the critical path, the GPU can process the augmentation so quickly that it isn't an issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8w1vYVjwQkG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "89839bbf-b4e6-41b7-bf9d-536a6181cc43"
      },
      "source": [
        "def measure_dataset_throughput(dataset_fn, label=\"\"):\n",
        "  count = 0\n",
        "  batch_size = 2048\n",
        "  for i, _ in enumerate(dataset_fn(batch_size=batch_size).take(50)):\n",
        "    if i == 3:\n",
        "      st = timeit.default_timer()\n",
        "    if i > 3:\n",
        "      count += 1\n",
        "  step_time = (timeit.default_timer() - st) / count\n",
        "  print(\"{:<45}  {:>6.0f} Images / sec\".format(label, batch_size / step_time // 100 * 100))\n",
        "\n",
        "\n",
        "def make_synthetic_dataset(batch_size, dtype=tf.float32, **kwargs):\n",
        "  dataset = tf.data.Dataset.range(2 * num_images_per_label)\n",
        "  def map_fn(_):\n",
        "    x = tf.zeros(shape=get_input_shape(), dtype=dtype)\n",
        "    y = tf.zeros(shape=(1,), dtype=dtype)\n",
        "    return x, y\n",
        "  dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "measure_dataset_throughput(\n",
        "    functools.partial(make_jpg_dataset, dtype=tf.float16),\n",
        "    \"Use JPEGs directly\")\n",
        "\n",
        "measure_dataset_throughput(\n",
        "    functools.partial(make_dataset, dtype=tf.float16),\n",
        "    \"Use TFRecords\")\n",
        "\n",
        "measure_dataset_throughput(\n",
        "    functools.partial(make_dataset, dtype=tf.float16, transform_on_device=True, \n",
        "                      already_resized=True),\n",
        "    \"Use TFRecords with scaling optimizations\")\n",
        "\n",
        "# Use synthetic data to ensure model is not input bound.\n",
        "measure_dataset_throughput(\n",
        "    functools.partial(make_synthetic_dataset, dtype=tf.float16),\n",
        "    \"Synthetic data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use JPEGs directly                               3200 Images / sec\n",
            "Use TFRecords                                    3500 Images / sec\n",
            "Use TFRecords with scaling optimizations        22800 Images / sec\n",
            "Synthetic data.                                 52200 Images / sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PzCrL832yhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "8d0335c0-7e21-418c-83d8-a495c9824bdb"
      },
      "source": [
        "for batch_size in [32, 64, 128]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(make_jpg_dataset, batch_size)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 32\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.19 sec\n",
            "Images / sec:      172\n",
            "\n",
            "Batch size: 64\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.24 sec\n",
            "Images / sec:      266\n",
            "\n",
            "Batch size: 128\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.37 sec\n",
            "Images / sec:      348\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVJnWI_OBJid"
      },
      "source": [
        "## Add tf.function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01mZRP8p9jny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "4c926ddc-12a0-45f2-e487-c62efdbf8c48"
      },
      "source": [
        "for batch_size in [32, 64, 128]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(make_jpg_dataset, batch_size, use_tf_function=True)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 32\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.10 sec\n",
            "Images / sec:      307\n",
            "\n",
            "Batch size: 64\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.20 sec\n",
            "Images / sec:      321\n",
            "\n",
            "Batch size: 128\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.41 sec\n",
            "Images / sec:      314\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rDLkJGIBNtU"
      },
      "source": [
        "## Add XLA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKKGxkTxasM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "fccf6aa2-f83c-4a73-c802-295cc6b273cb"
      },
      "source": [
        "for batch_size in [32 , 64, 128]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(make_jpg_dataset, batch_size, use_tf_function=True, xla=True)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 32\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.04 sec\n",
            "Images / sec:      817\n",
            "\n",
            "Batch size: 64\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.07 sec\n",
            "Images / sec:      930\n",
            "\n",
            "Batch size: 128\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.13 sec\n",
            "Images / sec:     1001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAIDkr-BQid"
      },
      "source": [
        "## Add mixed precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5CmKot0-Cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "55c81131-515a-43e2-bc76-a7c406185680"
      },
      "source": [
        "for batch_size in [32, 64, 128, 256]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(make_jpg_dataset, batch_size, use_tf_function=True, xla=True, mixed_precision=True)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 32\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.04 sec\n",
            "Images / sec:      813\n",
            "\n",
            "Batch size: 64\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.05 sec\n",
            "Images / sec:     1321\n",
            "\n",
            "Batch size: 128\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.08 sec\n",
            "Images / sec:     1653\n",
            "\n",
            "Batch size: 256\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.13 sec\n",
            "Images / sec:     1904\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FgCboNpBYi7"
      },
      "source": [
        "## Add distribution strategy and various tuning knobs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6YLvbXR3nB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "94032e63-ca5a-4db4-a31a-68c592096839"
      },
      "source": [
        "for batch_size in [256]:\n",
        "  print(\"Batch size: {}\".format(batch_size))\n",
        "  train_model(make_jpg_dataset, batch_size, strategy=tf.distribute.MirroredStrategy([\"GPU:0\"]), \n",
        "              use_tf_function=True, xla=True, mixed_precision=True, collect_profile=False)\n",
        "  print()\n",
        "\n",
        "  train_model(make_jpg_dataset, batch_size, strategy=tf.distribute.MirroredStrategy([\"GPU:0\"]), \n",
        "              use_tf_function=True, xla=True, mixed_precision=True, loss_scale=128, collect_profile=False)\n",
        "  print()\n",
        "\n",
        "  train_model(make_jpg_dataset, batch_size, strategy=tf.distribute.MirroredStrategy([\"GPU:0\"]), \n",
        "              use_tf_function=True, xla=True, mixed_precision=True, tuned=True, collect_profile=False)\n",
        "  print()\n",
        "\n",
        "  train_model(make_jpg_dataset, batch_size, strategy=tf.distribute.MirroredStrategy([\"GPU:0\"]), \n",
        "              use_tf_function=True, xla=True, mixed_precision=True, loss_scale=128, tuned=True, collect_profile=False)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 256\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.12 sec\n",
            "Images / sec:     2223\n",
            "\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.11 sec\n",
            "Images / sec:     2430\n",
            "\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.12 sec\n",
            "Images / sec:     2222\n",
            "\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.11 sec\n",
            "Images / sec:     2309\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zShqVRrGB1Aa"
      },
      "source": [
        "## Move to multi device\n",
        "\n",
        "In order for the input pipeline to keep up, we have to move to already resized images, and move the random augmentation onto the GPU, as the CPU cannot process the augmentation functions quickly enought even at 100% utilization. We also switch to using TFRecords instead of raw JPEGs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58wFWVDa-7kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "befb260c-5372-45bd-f554-7e1250886fc3"
      },
      "source": [
        "for num_gpus in [1, 2, 4, 8]:\n",
        "  for batch_size in [256 * num_gpus]:\n",
        "    print(\"Batch size: {}\".format(batch_size))\n",
        "    \n",
        "    train_model(functools.partial(make_dataset, already_resized=True), \n",
        "                batch_size, \n",
        "                strategy=tf.distribute.MirroredStrategy([\"GPU:{}\".format(i) for i in range(num_gpus)]), \n",
        "                use_tf_function=True, \n",
        "                xla=True, \n",
        "                mixed_precision=True,\n",
        "                transform_on_device=True,\n",
        "    )\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 256\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.11 sec\n",
            "Images / sec:     2268\n",
            "\n",
            "Batch size: 512\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.12 sec\n",
            "Images / sec:     4269\n",
            "\n",
            "Batch size: 1024\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.13 sec\n",
            "Images / sec:     7874\n",
            "\n",
            "Batch size: 2048\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "Burn in complete.\n",
            "30 steps\n",
            "Mean step time:   0.14 sec\n",
            "Images / sec:    14285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3k44OU66QXM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}